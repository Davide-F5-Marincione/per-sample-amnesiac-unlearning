\documentclass{article}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{overpic}
\usepackage{amssymb}

\usepackage[accepted]{dlai2024}

\begin{document}

\twocolumn[
%%% STUDENTS: FILL IN WITH YOUR OWN INFORMATION
\dlaititle{Per-Sample Amnesiac Unlearning}

\begin{center}\today\end{center}

\begin{dlaiauthorlist}
%%% STUDENTS: FILL IN WITH YOUR OWN INFORMATION
\dlaiauthor{Davide Marincione}{}
\end{dlaiauthorlist}

%%% STUDENTS: FILL IN WITH YOUR OWN INFORMATION
\dlaicorrespondingauthor{Davide Marincione}{marincione.1927757@studenti.uniroma1.it}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
Typical machine unlearning techniques do not scale well with modern deep learning models, as these either require models to abide to strict theoretical guarantees (lowering their effectiveness) or be retrained (unfeasible for big models). In this project we propose a variation of Amnesiac Unlearning, a technique which promises to bring unlearning with little to no compromises to neural networks. Our method lowers the memory requirements of the original technique while maintaining its effectiveness. We test it on MNIST and CIFAR-100, showing that it can make a model forget samples and classes while maintaining its performance on the rest of the dataset.
\end{abstract}
\section{Introduction}
Machine Unlearning is an ever-more important topic in the field of Machine Learning. Not only because privacy concerns regarding deep learning techniques are increasing, but also because regulations such as the EU's GDPR oblige companies to delete user's data at their request. Because of this, research in trying to produce an unlearning procedure that requires the less possible amount of retraining is increasing.

Our work consists of:
\begin{itemize}
    \item A new unlearning technique with lower memory requirements than the original Amnesiac Unlearning technique \cite{graves2021amnesiac}.
    \item Tests on MNIST and CIFAR-100 showing that our technique can make a model forget samples and classes while maintaining its performance on the rest of the dataset.
\end{itemize}

\section{Related Work}
As far as single-technique driven methods go (as combined procedures are often used in practice), the \emph{model shifting} \cite{vatter2023evolution} family of techniques is one of the most promising. These techniques are based on the idea of directly updating the model parameters to offset the impact of the samples to forget; by finding an update $\delta$ for $w_u=w+\delta$, where $w$ are the parameters of the original model. The reason for why this solution is so promising is that it does not require a major retraining of the model, which is often infeasible for big models: both in terms of time and computational resources.

One of the most successful \emph{model shifting} techniques is Amnesiac Unlearning \cite{graves2021amnesiac}, which defines the update $\delta$ with the gradient updates in which the samples to forget took part. To do so, the gradient update of each batch must be stored on disk, leading to a high memory requirement, as each update is of the same size as the model. This is a major drawback, as it makes the technique unfeasible for big models.
An extremely recent work \cite{gogineni2024efficient} proposes an approximation which only stores a random subset of the gradients in the update, therefore reducing the memory requirements. However, this leads to a decrease in the effectiveness of the technique, which requires a short retraining phase to completely forget the samples.

\section{Method}
Our method consists of storing the gradients not for each batch, but for each sample. On the long run, this leads to a lower memory requirement than the original technique, as we only need to store a single set of gradients for each sample (rather than a new set for each batch). If the space requirements of the original technique are $O(\frac{NME}{B})$, our technique requires $O(NMp)$ space, where $N$ is the number of samples in the dataset, $E$ is the number of epochs, $B$ is the batch size, $M$ is the number of parameters in the model, and $p$ is percentage of parameters kept in the random subset. It is easy to see that our method requires less space than the original technique if $p<\frac{E}{B}$ (which can be easily achieved in practice, as if we heuristically set $p=.1$ and have a typical $B=64$ then we just require $E=7$).

Not only can our method require less space than the original, but it also has the potential to be more effective than the approximation proposed in \cite{gogineni2024efficient}, as it only removes the gradients of the samples to forget, rather than the gradients of the full batch (where the influence of samples that are not to forget is removed as well). While still being slightly less effective than the original technique, even though it too uses the gradients of the batch.

\bibliography{references.bib}
\bibliographystyle{dlai2024}

\end{document}

