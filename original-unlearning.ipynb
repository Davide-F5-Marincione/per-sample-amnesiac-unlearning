{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nif not os.path.exists('steps'):\n    os.mkdir('steps')","metadata":{"_uuid":"a99b2ecd-d43b-4d60-867e-3fcb81058d24","_cell_guid":"18b8c3d0-01c7-4f24-a84a-0eb9e269118c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-17T17:42:36.395115Z","iopub.execute_input":"2024-07-17T17:42:36.395951Z","iopub.status.idle":"2024-07-17T17:42:36.401556Z","shell.execute_reply.started":"2024-07-17T17:42:36.395893Z","shell.execute_reply":"2024-07-17T17:42:36.400289Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torchvision import datasets, transforms, models\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport copy\nimport random\nimport time\nimport pickle\nfrom torch.nn.utils._per_sample_grad import call_for_per_sample_grads\n\ntorch.set_printoptions(precision=3)\ncuda = True if torch.cuda.is_available() else False","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:42:36.408421Z","iopub.execute_input":"2024-07-17T17:42:36.408838Z","iopub.status.idle":"2024-07-17T17:42:36.417194Z","shell.execute_reply.started":"2024-07-17T17:42:36.408803Z","shell.execute_reply":"2024-07-17T17:42:36.415998Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"to_forget = 81\nnum_classes = 100\nmax_count = -1\nin_size = 3\n\ntorch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:42:36.459435Z","iopub.execute_input":"2024-07-17T17:42:36.460170Z","iopub.status.idle":"2024-07-17T17:42:36.470058Z","shell.execute_reply.started":"2024-07-17T17:42:36.460133Z","shell.execute_reply":"2024-07-17T17:42:36.468816Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x79802124b2f0>"},"metadata":{}}]},{"cell_type":"code","source":"class IndexingDataset(Dataset):\n    def __init__(self, internal_dataset):\n        self.dataset = internal_dataset\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, sample_index):\n        r = self.dataset[sample_index]\n        if not isinstance(r, tuple):\n            r = (r,)\n        return *r, sample_index\n    \n# Transform image to tensor and normalize features from [0,255] to [0,1]\ntransform = transforms.Compose([transforms.ToTensor(), \n                                transforms.Normalize((0.5,),(0.5,)),\n                                ])\n\n# Using MNIST\ndata = datasets.CIFAR100('/data', download=True, train=True, transform=transform)\ntraindata = IndexingDataset(data)\ntestdata = datasets.CIFAR100('/data', download=True, train=False, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:42:36.486838Z","iopub.execute_input":"2024-07-17T17:42:36.488236Z","iopub.status.idle":"2024-07-17T17:42:52.538478Z","shell.execute_reply.started":"2024-07-17T17:42:36.488191Z","shell.execute_reply":"2024-07-17T17:42:52.537302Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:10<00:00, 15687434.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /data/cifar-100-python.tar.gz to /data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loaders that give 64 example batches\nall_data_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\nall_data_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64, shuffle=False)\n\ntarget_index = []\nnontarget_index = []\nfor i in range(0, len(testdata)):\n    if testdata[i][1] == to_forget:\n        target_index.append(i)\n    else:\n        nontarget_index.append(i)\ntarget_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64,\n              sampler = torch.utils.data.SubsetRandomSampler(target_index))\nnontarget_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64,\n              sampler = torch.utils.data.SubsetRandomSampler(nontarget_index))\n\ntarget_index = []\nnontarget_index = []\ncount = 0\nfor i in range(0, len(traindata)):\n    if traindata[i][1] != to_forget:\n        target_index.append(i)\n        nontarget_index.append(i)\n    if traindata[i][1] == to_forget and (count < max_count or max_count < 1):\n        count += 1\n        target_index.append(i)\ntarget_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64,\n                     sampler = torch.utils.data.SubsetRandomSampler(target_index))\nnontarget_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64,\n                     sampler = torch.utils.data.SubsetRandomSampler(nontarget_index))\n\n\nunlearningdata = copy.deepcopy(data)\nunlearninglabels = list(range(num_classes))\nunlearninglabels.remove(to_forget)\nfor i in range(len(unlearningdata)):\n    if unlearningdata.targets[i] == to_forget:\n        unlearningdata.targets[i] = random.choice(unlearninglabels)\nunlearning_train_loader = torch.utils.data.DataLoader(IndexingDataset(unlearningdata), batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:42:52.540942Z","iopub.execute_input":"2024-07-17T17:42:52.541873Z","iopub.status.idle":"2024-07-17T17:43:21.571188Z","shell.execute_reply.started":"2024-07-17T17:42:52.541826Z","shell.execute_reply":"2024-07-17T17:43:21.570146Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class SimpleModel(nn.Module):\n    def __init__(self, in_size, out_size, h_size=100):\n        super().__init__()\n        \n        self.in_size = in_size\n        self.out_size = out_size\n        self.h_size = h_size\n        \n        self.layers = nn.Sequential(\n            nn.Conv2d(in_size, h_size, 3, 2, padding=1),\n            nn.LeakyReLU(.1),\n            nn.Conv2d(h_size, h_size, 3, 2, padding=1),\n            nn.LeakyReLU(.1),\n            nn.AdaptiveMaxPool2d((2,2)),\n            nn.Flatten(1),\n            nn.Linear(4 * h_size, out_size)\n        )\n        \n        nn.init.xavier_normal_(self.layers[0].weight)\n        nn.init.zeros_(self.layers[0].bias)\n        nn.init.xavier_normal_(self.layers[2].weight)\n        nn.init.zeros_(self.layers[2].bias)\n        nn.init.xavier_normal_(self.layers[6].weight)\n        nn.init.zeros_(self.layers[6].bias)\n        \n    def forward(self, x):\n        return self.layers(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:43:21.572499Z","iopub.execute_input":"2024-07-17T17:43:21.572840Z","iopub.status.idle":"2024-07-17T17:43:21.581586Z","shell.execute_reply.started":"2024-07-17T17:43:21.572810Z","shell.execute_reply":"2024-07-17T17:43:21.580625Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nbatch_size_train = 64\nbatch_size_test = 64\nlog_interval = 16\nP=.1\ntorch.backends.cudnn.enabled = True\ncriterion = F.cross_entropy","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:43:21.583860Z","iopub.execute_input":"2024-07-17T17:43:21.584172Z","iopub.status.idle":"2024-07-17T17:43:21.640506Z","shell.execute_reply.started":"2024-07-17T17:43:21.584149Z","shell.execute_reply":"2024-07-17T17:43:21.639406Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Training method\ndef train(model, epoch, loader, returnable=False, keep_p=.1):\n    model.train()\n    if returnable:\n        batches = []\n    for batch_idx, (data, target, samples_idx) in enumerate(loader):\n        optimizer.zero_grad()\n        if to_forget in target:\n            before = {}\n            for key, param in model.named_parameters():\n                before[key] = param.clone()\n        data = data.to(device)\n        output = model(data)\n        loss = criterion(output, target.to(device))\n        loss.backward()\n        \n        optimizer.step()\n        \n        with torch.no_grad():\n            if to_forget in target:\n                batches.append(batch_idx)\n                step = {}\n                for key, param in model.named_parameters():\n                    step[key] = (param - before[key]).cpu()\n                f = open(f\"steps/e{epoch}b{batches[-1]:04}.pkl\", \"wb\")\n                pickle.dump(step, f)\n                f.close()\n        if batch_idx % log_interval == 0:\n            print(\"\\rEpoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n              epoch, batch_idx*len(data),  loss.item()), end=\"\")\n    if returnable:\n        return batches","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:43:21.642855Z","iopub.execute_input":"2024-07-17T17:43:21.643366Z","iopub.status.idle":"2024-07-17T17:43:21.659601Z","shell.execute_reply.started":"2024-07-17T17:43:21.643298Z","shell.execute_reply":"2024-07-17T17:43:21.658551Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Testing method\ndef test(model, loader, dname=\"Test set\", printable=True):\n    model.eval()\n    test_loss = 0\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in loader:\n            data = data.to(device)\n            target = target.to(device)\n            output = model(data)\n            total += target.size()[0]\n            test_loss += criterion(output, target).item()\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.data.view_as(pred)).sum()\n    test_loss /= len(loader.dataset)\n    if printable:\n        print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n            dname, test_loss, correct, total, \n            100. * correct / total\n            ))\n    return 1. * correct / total","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:43:21.661247Z","iopub.execute_input":"2024-07-17T17:43:21.662538Z","iopub.status.idle":"2024-07-17T17:43:21.674387Z","shell.execute_reply.started":"2024-07-17T17:43:21.662497Z","shell.execute_reply":"2024-07-17T17:43:21.673116Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"trainingepochs = 4\nforgetfulepochs = 4","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:43:21.676011Z","iopub.execute_input":"2024-07-17T17:43:21.676712Z","iopub.status.idle":"2024-07-17T17:43:21.684179Z","shell.execute_reply.started":"2024-07-17T17:43:21.676677Z","shell.execute_reply":"2024-07-17T17:43:21.682550Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# load resnet 18 and change to fit problem dimensionality\n#model = models.resnet18()\n#model.bn1 = nn.GroupNorm(1, model.bn1.weight.shape[0])\n#model.layer1[0].bn1 = nn.GroupNorm(1, model.layer1[0].bn1.weight.shape[0])\n#model.layer1[0].bn2 = nn.GroupNorm(1, model.layer1[0].bn2.weight.shape[0])\n#model.layer1[1].bn1 = nn.GroupNorm(1, model.layer1[1].bn1.weight.shape[0])\n#model.layer1[1].bn2 = nn.GroupNorm(1, model.layer1[1].bn2.weight.shape[0])\n\n#model.layer2[0].bn1 = nn.GroupNorm(1, model.layer2[0].bn1.weight.shape[0])\n#model.layer2[0].bn2 = nn.GroupNorm(1, model.layer2[0].bn2.weight.shape[0])\n#model.layer2[0].downsample[1] = nn.GroupNorm(1, model.layer2[0].downsample[1].weight.shape[0])\n#model.layer2[1].bn1 = nn.GroupNorm(1, model.layer2[1].bn1.weight.shape[0])\n#model.layer2[1].bn2 = nn.GroupNorm(1, model.layer2[1].bn2.weight.shape[0])\n\n#model.layer3[0].bn1 = nn.GroupNorm(1, model.layer3[0].bn1.weight.shape[0])\n#model.layer3[0].bn2 = nn.GroupNorm(1, model.layer3[0].bn2.weight.shape[0])\n#odel.layer3[0].downsample[1] = nn.GroupNorm(1, model.layer3[0].downsample[1].weight.shape[0])\n#model.layer3[1].bn1 = nn.GroupNorm(1, model.layer3[1].bn1.weight.shape[0])\n#model.layer3[1].bn2 = nn.GroupNorm(1, model.layer3[1].bn2.weight.shape[0])\n\n#model.layer4[0].bn1 = nn.GroupNorm(1, model.layer4[0].bn1.weight.shape[0])\n#model.layer4[0].bn2 = nn.GroupNorm(1, model.layer4[0].bn2.weight.shape[0])\n#model.layer4[0].downsample[1] = nn.GroupNorm(1, model.layer4[0].downsample[1].weight.shape[0])\n#model.layer4[1].bn1 = nn.GroupNorm(1, model.layer4[1].bn1.weight.shape[0])\n#model.layer4[1].bn2 = nn.GroupNorm(1, model.layer4[1].bn2.weight.shape[0])\n\n#model.conv1 = nn.Conv2d(in_size, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n#model.fc = nn.Sequential(nn.Linear(512, num_classes))\n\ndevice = \"cuda\" if torch.cuda.is_available() else 'cpu'\n\nmodel = SimpleModel(in_size, num_classes)\n\nmodel = model.to(device)\n\noptimizer = optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:43:21.687707Z","iopub.execute_input":"2024-07-17T17:43:21.688206Z","iopub.status.idle":"2024-07-17T17:43:21.704233Z","shell.execute_reply.started":"2024-07-17T17:43:21.688163Z","shell.execute_reply":"2024-07-17T17:43:21.703228Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Train new model for 5 epochs\nepoch_indices = []\nfor epoch in range(1, trainingepochs+1):\n    starttime = time.process_time()\n    # train(resnet, epoch, all_data_train_loader, returnable=False)\n    batches = train(model, epoch, target_train_loader, returnable=True, keep_p=P)\n    print(f\"{batches} batches effected\")\n    epoch_indices.append(batches)\n    test(model, all_data_test_loader, dname=\"All data\")\n    test(model, target_test_loader, dname=\"Forget  \")\n    test(model, nontarget_test_loader, dname=\"Retain  \")\n    print(f\"Time taken: {time.process_time() - starttime}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:43:21.706613Z","iopub.execute_input":"2024-07-17T17:43:21.706986Z","iopub.status.idle":"2024-07-17T17:45:04.195508Z","shell.execute_reply.started":"2024-07-17T17:43:21.706947Z","shell.execute_reply":"2024-07-17T17:45:04.194353Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch: 1 [ 49152]\tLoss: 3.184155[1, 2, 4, 8, 9, 10, 17, 19, 20, 22, 23, 25, 26, 27, 28, 33, 34, 36, 39, 45, 47, 51, 53, 54, 56, 57, 60, 62, 64, 65, 68, 70, 74, 76, 77, 84, 85, 86, 88, 89, 90, 91, 92, 93, 98, 100, 101, 102, 106, 110, 111, 116, 118, 119, 120, 121, 124, 125, 128, 129, 132, 134, 137, 138, 140, 141, 144, 148, 153, 154, 159, 160, 162, 165, 166, 169, 170, 172, 173, 177, 180, 182, 183, 184, 186, 188, 189, 192, 193, 196, 198, 200, 201, 203, 204, 205, 206, 210, 211, 213, 215, 217, 218, 219, 220, 221, 222, 224, 228, 232, 239, 242, 251, 252, 254, 256, 258, 259, 261, 267, 270, 274, 275, 276, 277, 281, 282, 284, 286, 287, 289, 290, 291, 293, 294, 295, 297, 299, 303, 304, 306, 307, 308, 310, 312, 314, 315, 316, 317, 318, 319, 322, 324, 325, 332, 333, 335, 336, 337, 338, 340, 342, 347, 349, 350, 352, 354, 357, 358, 360, 362, 363, 364, 366, 367, 370, 371, 372, 373, 374, 376, 377, 379, 380, 381, 382, 384, 385, 386, 390, 391, 394, 395, 398, 399, 400, 401, 404, 407, 410, 411, 412, 415, 417, 418, 419, 421, 422, 423, 425, 426, 427, 429, 430, 433, 434, 435, 443, 444, 445, 447, 448, 449, 450, 452, 455, 458, 460, 462, 466, 467, 468, 469, 471, 472, 476, 478, 482, 488, 490, 492, 495, 497, 499, 501, 502, 503, 504, 506, 507, 510, 511, 512, 513, 517, 518, 522, 523, 525, 527, 529, 534, 540, 541, 544, 545, 546, 551, 554, 555, 556, 558, 559, 560, 562, 567, 570, 571, 574, 582, 585, 586, 589, 592, 596, 597, 598, 600, 602, 603, 604, 606, 609, 610, 613, 615, 616, 617, 618, 620, 621, 622, 626, 628, 629, 630, 631, 640, 645, 646, 649, 651, 653, 654, 655, 656, 657, 662, 666, 668, 671, 675, 676, 678, 680, 681, 682, 683, 684, 685, 686, 687, 694, 695, 699, 705, 708, 710, 713, 717, 718, 721, 722, 724, 725, 726, 727, 728, 730, 733, 734, 735, 737, 738, 740, 745, 746, 747, 749, 751, 754, 755, 756, 758, 759, 760, 762, 767, 768, 770, 772, 773, 774, 778, 779, 780] batches effected\nAll data: Mean loss: 0.0522, Accuracy: 2167/10000 (22%)\nForget  : Mean loss: 0.0006, Accuracy: 15/100 (15%)\nRetain  : Mean loss: 0.0516, Accuracy: 2152/9900 (22%)\nTime taken: 25.834312694999994\nEpoch: 2 [ 49152]\tLoss: 2.799393[0, 1, 3, 5, 6, 7, 9, 11, 12, 13, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 33, 38, 39, 41, 42, 44, 46, 47, 48, 49, 50, 52, 53, 55, 58, 60, 61, 63, 65, 66, 68, 70, 73, 74, 75, 76, 78, 79, 80, 84, 86, 88, 90, 92, 95, 96, 98, 102, 103, 104, 107, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 122, 123, 128, 129, 130, 132, 134, 137, 142, 146, 149, 151, 152, 153, 154, 160, 162, 163, 164, 166, 170, 173, 174, 179, 181, 187, 189, 190, 192, 194, 195, 197, 198, 199, 200, 201, 202, 205, 207, 208, 210, 214, 216, 217, 219, 222, 223, 225, 226, 228, 230, 231, 233, 237, 238, 239, 244, 245, 248, 249, 250, 251, 252, 253, 257, 258, 260, 261, 263, 265, 267, 268, 269, 275, 278, 286, 287, 289, 291, 292, 294, 295, 298, 299, 301, 302, 306, 307, 309, 310, 312, 315, 316, 317, 319, 322, 324, 325, 328, 330, 331, 334, 335, 339, 341, 342, 343, 344, 349, 350, 352, 354, 355, 357, 359, 360, 361, 362, 366, 367, 368, 369, 370, 374, 376, 378, 379, 382, 387, 388, 390, 392, 401, 403, 405, 406, 407, 409, 412, 413, 419, 422, 423, 427, 430, 431, 434, 435, 437, 440, 442, 445, 449, 450, 452, 457, 458, 460, 461, 463, 465, 466, 467, 471, 474, 475, 476, 477, 479, 482, 489, 490, 491, 495, 498, 499, 501, 503, 505, 506, 507, 508, 511, 513, 515, 516, 519, 522, 525, 530, 532, 533, 534, 536, 537, 538, 539, 540, 543, 545, 546, 547, 549, 550, 551, 552, 554, 555, 557, 559, 565, 567, 568, 571, 572, 574, 576, 577, 578, 582, 584, 585, 587, 589, 593, 594, 596, 598, 599, 603, 604, 605, 609, 613, 614, 623, 627, 628, 630, 633, 634, 638, 640, 648, 650, 651, 652, 653, 654, 657, 658, 664, 669, 672, 676, 677, 680, 683, 688, 690, 691, 692, 694, 701, 702, 710, 713, 715, 718, 721, 722, 723, 731, 732, 737, 738, 739, 741, 745, 747, 749, 750, 754, 757, 759, 760, 763, 765, 769, 770, 771, 772, 774, 775] batches effected\nAll data: Mean loss: 0.0466, Accuracy: 2837/10000 (28%)\nForget  : Mean loss: 0.0006, Accuracy: 18/100 (18%)\nRetain  : Mean loss: 0.0460, Accuracy: 2819/9900 (28%)\nTime taken: 25.790983464000078\nEpoch: 3 [ 49152]\tLoss: 2.639815[0, 6, 7, 8, 10, 11, 13, 16, 18, 19, 21, 24, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39, 40, 42, 43, 44, 48, 52, 56, 62, 63, 64, 66, 67, 68, 70, 73, 74, 76, 81, 85, 86, 91, 92, 94, 95, 96, 100, 102, 103, 104, 106, 113, 116, 124, 126, 128, 130, 132, 134, 138, 139, 141, 142, 143, 144, 148, 149, 154, 159, 161, 162, 163, 166, 167, 168, 170, 171, 172, 173, 174, 175, 178, 179, 182, 183, 185, 188, 189, 195, 196, 198, 199, 201, 203, 204, 207, 208, 209, 218, 219, 221, 226, 228, 232, 234, 237, 239, 240, 241, 242, 246, 248, 256, 259, 263, 266, 271, 273, 274, 275, 284, 286, 290, 293, 295, 297, 298, 299, 301, 303, 306, 307, 310, 312, 313, 317, 318, 322, 323, 325, 331, 332, 335, 336, 338, 340, 343, 344, 345, 348, 350, 351, 352, 353, 356, 357, 359, 363, 364, 366, 368, 369, 374, 379, 384, 385, 386, 388, 389, 390, 392, 395, 396, 397, 399, 401, 402, 404, 406, 410, 413, 415, 416, 420, 422, 428, 433, 439, 440, 441, 443, 447, 450, 451, 453, 455, 459, 460, 464, 465, 467, 469, 471, 473, 475, 478, 481, 482, 483, 484, 488, 489, 490, 491, 492, 498, 502, 503, 506, 507, 509, 510, 511, 513, 514, 516, 518, 519, 520, 521, 522, 524, 525, 528, 530, 532, 535, 538, 539, 543, 544, 545, 546, 547, 548, 550, 551, 552, 561, 562, 571, 575, 576, 578, 580, 581, 582, 585, 588, 589, 591, 592, 594, 595, 596, 601, 602, 603, 607, 608, 609, 613, 614, 615, 619, 620, 621, 623, 624, 630, 634, 638, 640, 642, 647, 648, 649, 650, 653, 654, 655, 657, 658, 660, 664, 665, 669, 671, 672, 674, 675, 676, 679, 680, 681, 682, 683, 688, 689, 691, 692, 696, 697, 699, 700, 701, 702, 703, 706, 709, 710, 712, 715, 716, 718, 719, 721, 723, 724, 725, 726, 728, 729, 733, 735, 736, 738, 741, 742, 743, 744, 745, 748, 749, 752, 755, 756, 757, 758, 761, 762, 764, 766, 768, 769, 770, 771, 772, 773, 776, 777, 778] batches effected\nAll data: Mean loss: 0.0447, Accuracy: 3059/10000 (31%)\nForget  : Mean loss: 0.0006, Accuracy: 24/100 (24%)\nRetain  : Mean loss: 0.0441, Accuracy: 3035/9900 (31%)\nTime taken: 25.538609663999978\nEpoch: 4 [ 49152]\tLoss: 2.454937[0, 3, 6, 7, 8, 9, 16, 17, 18, 19, 20, 23, 27, 28, 30, 36, 38, 40, 41, 44, 46, 51, 53, 55, 59, 62, 63, 64, 65, 67, 68, 70, 72, 73, 77, 78, 81, 84, 85, 87, 94, 95, 96, 98, 99, 103, 105, 106, 109, 113, 115, 119, 120, 122, 124, 128, 135, 139, 140, 143, 144, 148, 153, 155, 156, 158, 159, 161, 163, 168, 169, 170, 172, 174, 176, 178, 179, 180, 181, 183, 184, 185, 186, 191, 192, 194, 195, 196, 197, 199, 201, 202, 203, 206, 208, 213, 214, 215, 218, 221, 222, 223, 224, 228, 229, 230, 237, 238, 240, 249, 252, 253, 254, 256, 257, 258, 259, 262, 264, 265, 266, 269, 270, 271, 272, 273, 275, 278, 279, 280, 281, 290, 291, 293, 297, 299, 301, 303, 304, 305, 306, 307, 309, 310, 311, 313, 314, 315, 316, 317, 318, 320, 326, 328, 332, 333, 335, 339, 341, 347, 348, 350, 353, 356, 357, 360, 365, 370, 373, 374, 375, 377, 379, 381, 383, 384, 387, 388, 392, 394, 395, 397, 399, 402, 403, 411, 417, 419, 420, 421, 422, 424, 425, 426, 428, 430, 431, 435, 437, 438, 440, 441, 447, 448, 449, 450, 452, 456, 461, 462, 463, 464, 468, 470, 472, 473, 474, 476, 478, 481, 483, 484, 485, 489, 493, 494, 496, 501, 503, 504, 506, 507, 512, 516, 517, 518, 519, 520, 521, 522, 523, 525, 530, 535, 538, 539, 540, 545, 548, 551, 552, 554, 557, 561, 562, 566, 567, 568, 571, 573, 575, 577, 579, 580, 582, 585, 586, 588, 589, 593, 597, 598, 600, 602, 609, 611, 613, 615, 616, 618, 619, 621, 622, 625, 626, 627, 629, 630, 631, 637, 641, 645, 646, 647, 648, 649, 652, 655, 656, 658, 663, 664, 667, 668, 669, 670, 671, 676, 677, 678, 681, 684, 685, 687, 688, 689, 690, 691, 692, 694, 696, 697, 698, 700, 701, 703, 704, 706, 707, 709, 712, 713, 716, 717, 719, 720, 723, 729, 733, 735, 736, 738, 741, 744, 745, 746, 747, 748, 750, 751, 759, 762, 764, 765, 767, 769, 776, 778, 779, 780, 781] batches effected\nAll data: Mean loss: 0.0431, Accuracy: 3293/10000 (33%)\nForget  : Mean loss: 0.0005, Accuracy: 21/100 (21%)\nRetain  : Mean loss: 0.0425, Accuracy: 3272/9900 (33%)\nTime taken: 25.28267047099996\n","output_type":"stream"}]},{"cell_type":"code","source":"path = F\"selective_trained.pt\"\ntorch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, path)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:45:04.198949Z","iopub.execute_input":"2024-07-17T17:45:04.199302Z","iopub.status.idle":"2024-07-17T17:45:04.212118Z","shell.execute_reply.started":"2024-07-17T17:45:04.199272Z","shell.execute_reply":"2024-07-17T17:45:04.211007Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"path = F\"selective_trained.pt\"\ncheckpoint = torch.load(path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:45:04.213377Z","iopub.execute_input":"2024-07-17T17:45:04.213708Z","iopub.status.idle":"2024-07-17T17:45:04.229257Z","shell.execute_reply.started":"2024-07-17T17:45:04.213680Z","shell.execute_reply":"2024-07-17T17:45:04.228206Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"for i in range(1, trainingepochs+1):\n    for j in range(1600):\n        path = f\"steps/e{i}b{j:04}.pkl\"\n        try:\n            f = open(path, \"rb\")\n            steps = pickle.load(f)\n            f.close()\n            print(f\"\\rLoading steps/e{i}b{j:04}.pkl\", end=\"\")\n            const = 1\n            with torch.no_grad():\n                for key, param in model.named_parameters():\n                    param -= const*steps[key].to(device)\n        except:\n#             print(f\"\\r{i},{j}\", end=\"\")\n            pass\nprint(steps.keys())","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:45:04.230884Z","iopub.execute_input":"2024-07-17T17:45:04.231247Z","iopub.status.idle":"2024-07-17T17:45:19.250463Z","shell.execute_reply.started":"2024-07-17T17:45:04.231218Z","shell.execute_reply":"2024-07-17T17:45:19.249287Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Loading steps/e4b0842.pkldict_keys(['layers.0.weight', 'layers.0.bias', 'layers.2.weight', 'layers.2.bias', 'layers.6.weight', 'layers.6.bias'])\n","output_type":"stream"}]},{"cell_type":"code","source":"test(model, all_data_test_loader, dname=\"All data\")\ntest(model, target_test_loader, dname=\"Forget  \")\ntest(model, nontarget_test_loader, dname=\"Retain  \")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:45:19.252842Z","iopub.execute_input":"2024-07-17T17:45:19.253326Z","iopub.status.idle":"2024-07-17T17:45:25.490716Z","shell.execute_reply.started":"2024-07-17T17:45:19.253280Z","shell.execute_reply":"2024-07-17T17:45:25.489685Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"All data: Mean loss: 0.0819, Accuracy: 506/10000 (5%)\nForget  : Mean loss: 0.0055, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0773, Accuracy: 506/9900 (5%)\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"tensor(0.051, device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"path = F\"selective_post_trained.pt\"\ntorch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, path)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:45:25.492151Z","iopub.execute_input":"2024-07-17T17:45:25.492513Z","iopub.status.idle":"2024-07-17T17:45:25.504833Z","shell.execute_reply.started":"2024-07-17T17:45:25.492471Z","shell.execute_reply":"2024-07-17T17:45:25.503927Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"path = F\"selective_post_trained.pt\"\ncheckpoint = torch.load(path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:45:25.506385Z","iopub.execute_input":"2024-07-17T17:45:25.506774Z","iopub.status.idle":"2024-07-17T17:45:25.520757Z","shell.execute_reply.started":"2024-07-17T17:45:25.506739Z","shell.execute_reply":"2024-07-17T17:45:25.519971Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Train model for 10 epochs\nfor epoch in range(trainingepochs+1,trainingepochs+forgetfulepochs+1):\n  # train(resnet, epoch, nonthree_train_loader, returnable=False)\n    _ = train(model, epoch, nontarget_train_loader, returnable=True)\n    test(model, all_data_test_loader, dname=\"All data\")\n    test(model, target_test_loader, dname=\"Forget  \")\n    test(model, nontarget_test_loader, dname=\"Retain  \")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T17:45:25.521907Z","iopub.execute_input":"2024-07-17T17:45:25.522245Z","iopub.status.idle":"2024-07-17T17:46:58.335770Z","shell.execute_reply.started":"2024-07-17T17:45:25.522199Z","shell.execute_reply":"2024-07-17T17:46:58.334648Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Epoch: 5 [ 49152]\tLoss: 2.372996All data: Mean loss: 0.0481, Accuracy: 3129/10000 (31%)\nForget  : Mean loss: 0.0056, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0435, Accuracy: 3129/9900 (32%)\nEpoch: 6 [ 49152]\tLoss: 2.968864All data: Mean loss: 0.0467, Accuracy: 3300/10000 (33%)\nForget  : Mean loss: 0.0058, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0421, Accuracy: 3300/9900 (33%)\nEpoch: 7 [ 49152]\tLoss: 2.371677All data: Mean loss: 0.0457, Accuracy: 3424/10000 (34%)\nForget  : Mean loss: 0.0057, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0411, Accuracy: 3424/9900 (35%)\nEpoch: 8 [ 49152]\tLoss: 2.189278All data: Mean loss: 0.0449, Accuracy: 3532/10000 (35%)\nForget  : Mean loss: 0.0055, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0404, Accuracy: 3532/9900 (36%)\n","output_type":"stream"}]}]}