{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nif not os.path.exists('steps'):\n    os.mkdir('steps')","metadata":{"_uuid":"a99b2ecd-d43b-4d60-867e-3fcb81058d24","_cell_guid":"18b8c3d0-01c7-4f24-a84a-0eb9e269118c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-17T20:03:29.933016Z","iopub.execute_input":"2024-07-17T20:03:29.933600Z","iopub.status.idle":"2024-07-17T20:03:29.945262Z","shell.execute_reply.started":"2024-07-17T20:03:29.933573Z","shell.execute_reply":"2024-07-17T20:03:29.944337Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torchvision import datasets, transforms, models\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport copy\nimport random\nimport time\nimport pickle\nfrom torch.nn.utils._per_sample_grad import call_for_per_sample_grads\n\ntorch.set_printoptions(precision=3)\ncuda = True if torch.cuda.is_available() else False","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:03:29.947129Z","iopub.execute_input":"2024-07-17T20:03:29.947388Z","iopub.status.idle":"2024-07-17T20:03:35.408709Z","shell.execute_reply.started":"2024-07-17T20:03:29.947365Z","shell.execute_reply":"2024-07-17T20:03:35.407755Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"to_forget = 81\nnum_classes = 100\nmax_count = -1\nin_size = 3\n\ntorch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:03:35.409961Z","iopub.execute_input":"2024-07-17T20:03:35.410339Z","iopub.status.idle":"2024-07-17T20:03:35.420398Z","shell.execute_reply.started":"2024-07-17T20:03:35.410315Z","shell.execute_reply":"2024-07-17T20:03:35.419539Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7ae59580f310>"},"metadata":{}}]},{"cell_type":"code","source":"class IndexingDataset(Dataset):\n    def __init__(self, internal_dataset):\n        self.dataset = internal_dataset\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, sample_index):\n        r = self.dataset[sample_index]\n        if not isinstance(r, tuple):\n            r = (r,)\n        return *r, sample_index\n    \n# Transform image to tensor and normalize features from [0,255] to [0,1]\ntransform = transforms.Compose([transforms.ToTensor(), \n                                transforms.Normalize((0.5,),(0.5,)),\n                                ])\n\n# Using MNIST\ndata = datasets.CIFAR100('/data', download=True, train=True, transform=transform)\ntraindata = IndexingDataset(data)\ntestdata = datasets.CIFAR100('/data', download=True, train=False, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:03:35.422742Z","iopub.execute_input":"2024-07-17T20:03:35.423161Z","iopub.status.idle":"2024-07-17T20:03:40.733610Z","shell.execute_reply.started":"2024-07-17T20:03:35.423130Z","shell.execute_reply":"2024-07-17T20:03:40.732590Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:01<00:00, 105732000.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /data/cifar-100-python.tar.gz to /data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loaders that give 64 example batches\nall_data_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\nall_data_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64, shuffle=False)\n\ntarget_index = []\nnontarget_index = []\nfor i in range(0, len(testdata)):\n    if testdata[i][1] == to_forget:\n        target_index.append(i)\n    else:\n        nontarget_index.append(i)\ntarget_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64,\n              sampler = torch.utils.data.SubsetRandomSampler(target_index))\nnontarget_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64,\n              sampler = torch.utils.data.SubsetRandomSampler(nontarget_index))\n\ntarget_index = []\nnontarget_index = []\ncount = 0\nfor i in range(0, len(traindata)):\n    if traindata[i][1] != to_forget:\n        target_index.append(i)\n        nontarget_index.append(i)\n    if traindata[i][1] == to_forget and (count < max_count or max_count < 1):\n        count += 1\n        target_index.append(i)\ntarget_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64,\n                     sampler = torch.utils.data.SubsetRandomSampler(target_index))\nnontarget_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64,\n                     sampler = torch.utils.data.SubsetRandomSampler(nontarget_index))\n\n\nunlearningdata = copy.deepcopy(data)\nunlearninglabels = list(range(num_classes))\nunlearninglabels.remove(to_forget)\nfor i in range(len(unlearningdata)):\n    if unlearningdata.targets[i] == to_forget:\n        unlearningdata.targets[i] = random.choice(unlearninglabels)\nunlearning_train_loader = torch.utils.data.DataLoader(IndexingDataset(unlearningdata), batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:03:40.734830Z","iopub.execute_input":"2024-07-17T20:03:40.735134Z","iopub.status.idle":"2024-07-17T20:04:05.598258Z","shell.execute_reply.started":"2024-07-17T20:03:40.735108Z","shell.execute_reply":"2024-07-17T20:04:05.597469Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class SimpleModel(nn.Module):\n    def __init__(self, in_size, out_size, h_size=100):\n        super().__init__()\n        \n        self.in_size = in_size\n        self.out_size = out_size\n        self.h_size = h_size\n        \n        self.layers = nn.Sequential(\n            nn.Conv2d(in_size, h_size, 3, 2, padding=1),\n            nn.LeakyReLU(.1),\n            nn.Conv2d(h_size, h_size, 3, 2, padding=1),\n            nn.LeakyReLU(.1),\n            nn.AdaptiveMaxPool2d((2,2)),\n            nn.Flatten(1),\n            nn.Linear(4 * h_size, out_size)\n        )\n        \n        nn.init.xavier_normal_(self.layers[0].weight)\n        nn.init.zeros_(self.layers[0].bias)\n        nn.init.xavier_normal_(self.layers[2].weight)\n        nn.init.zeros_(self.layers[2].bias)\n        nn.init.xavier_normal_(self.layers[6].weight)\n        nn.init.zeros_(self.layers[6].bias)\n        \n    def forward(self, x):\n        return self.layers(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:04:05.599451Z","iopub.execute_input":"2024-07-17T20:04:05.599796Z","iopub.status.idle":"2024-07-17T20:04:05.608903Z","shell.execute_reply.started":"2024-07-17T20:04:05.599767Z","shell.execute_reply":"2024-07-17T20:04:05.607850Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nbatch_size_train = 64\nbatch_size_test = 64\nlog_interval = 16\nP=.1\ntorch.backends.cudnn.enabled = True\ncriterion = F.cross_entropy","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:04:05.610091Z","iopub.execute_input":"2024-07-17T20:04:05.610407Z","iopub.status.idle":"2024-07-17T20:04:05.622131Z","shell.execute_reply.started":"2024-07-17T20:04:05.610375Z","shell.execute_reply":"2024-07-17T20:04:05.621174Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Training method\nstuff = {}\ndef train(model, epoch, loader, returnable=False, keep_p=.1):\n    model.train()\n    rng = np.random.default_rng(42)\n    if returnable:\n        batches = []\n    for batch_idx, (data, target, samples_idx) in enumerate(loader):\n        optimizer.zero_grad()\n        if to_forget in target:\n            before = {}\n            for key, param in model.named_parameters():\n                before[key] = param.clone()\n        data = data.to(device)\n        output = model(data)\n        loss = criterion(output, target.to(device))\n        loss.backward()\n        \n        optimizer.step()\n        \n        with torch.no_grad():\n            if to_forget in target:\n                batches.append(batch_idx)\n                step = {}\n                for key, param in model.named_parameters():\n                    if key not in stuff:\n                        stuff[key] = torch.zeros(param.shape, device='cpu')\n                    diff = (param - before[key]).cpu().flatten()\n                    size = diff.shape.numel()\n                    subset = rng.choice(size, int(size * keep_p), replace=False, shuffle=False)\n                    stuff[key].view(-1)[subset] += diff[subset]\n                #torch.save(step, f\"steps/e{epoch}b{batches[-1]:04}.pkl\")\n        if batch_idx % log_interval == 0:\n            print(\"\\rEpoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n              epoch, batch_idx*len(data),  loss.item()), end=\"\")\n    if returnable:\n        return batches","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:04:05.623180Z","iopub.execute_input":"2024-07-17T20:04:05.623467Z","iopub.status.idle":"2024-07-17T20:04:05.636401Z","shell.execute_reply.started":"2024-07-17T20:04:05.623443Z","shell.execute_reply":"2024-07-17T20:04:05.635347Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Testing method\ndef test(model, loader, dname=\"Test set\", printable=True):\n    model.eval()\n    test_loss = 0\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in loader:\n            data = data.to(device)\n            target = target.to(device)\n            output = model(data)\n            total += target.size()[0]\n            test_loss += criterion(output, target).item()\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.data.view_as(pred)).sum()\n    test_loss /= len(loader.dataset)\n    if printable:\n        print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n            dname, test_loss, correct, total, \n            100. * correct / total\n            ))\n    return 1. * correct / total","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:04:05.639965Z","iopub.execute_input":"2024-07-17T20:04:05.640207Z","iopub.status.idle":"2024-07-17T20:04:05.648865Z","shell.execute_reply.started":"2024-07-17T20:04:05.640185Z","shell.execute_reply":"2024-07-17T20:04:05.647948Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainingepochs = 4\nforgetfulepochs = 4","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:04:05.649897Z","iopub.execute_input":"2024-07-17T20:04:05.650167Z","iopub.status.idle":"2024-07-17T20:04:05.659838Z","shell.execute_reply.started":"2024-07-17T20:04:05.650146Z","shell.execute_reply":"2024-07-17T20:04:05.658944Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# load resnet 18 and change to fit problem dimensionality\nmodel = models.resnet18()\nmodel.bn1 = nn.GroupNorm(1, model.bn1.weight.shape[0])\nmodel.layer1[0].bn1 = nn.GroupNorm(1, model.layer1[0].bn1.weight.shape[0])\nmodel.layer1[0].bn2 = nn.GroupNorm(1, model.layer1[0].bn2.weight.shape[0])\nmodel.layer1[1].bn1 = nn.GroupNorm(1, model.layer1[1].bn1.weight.shape[0])\nmodel.layer1[1].bn2 = nn.GroupNorm(1, model.layer1[1].bn2.weight.shape[0])\n\nmodel.layer2[0].bn1 = nn.GroupNorm(1, model.layer2[0].bn1.weight.shape[0])\nmodel.layer2[0].bn2 = nn.GroupNorm(1, model.layer2[0].bn2.weight.shape[0])\nmodel.layer2[0].downsample[1] = nn.GroupNorm(1, model.layer2[0].downsample[1].weight.shape[0])\nmodel.layer2[1].bn1 = nn.GroupNorm(1, model.layer2[1].bn1.weight.shape[0])\nmodel.layer2[1].bn2 = nn.GroupNorm(1, model.layer2[1].bn2.weight.shape[0])\n\nmodel.layer3[0].bn1 = nn.GroupNorm(1, model.layer3[0].bn1.weight.shape[0])\nmodel.layer3[0].bn2 = nn.GroupNorm(1, model.layer3[0].bn2.weight.shape[0])\nmodel.layer3[0].downsample[1] = nn.GroupNorm(1, model.layer3[0].downsample[1].weight.shape[0])\nmodel.layer3[1].bn1 = nn.GroupNorm(1, model.layer3[1].bn1.weight.shape[0])\nmodel.layer3[1].bn2 = nn.GroupNorm(1, model.layer3[1].bn2.weight.shape[0])\n\nmodel.layer4[0].bn1 = nn.GroupNorm(1, model.layer4[0].bn1.weight.shape[0])\nmodel.layer4[0].bn2 = nn.GroupNorm(1, model.layer4[0].bn2.weight.shape[0])\nmodel.layer4[0].downsample[1] = nn.GroupNorm(1, model.layer4[0].downsample[1].weight.shape[0])\nmodel.layer4[1].bn1 = nn.GroupNorm(1, model.layer4[1].bn1.weight.shape[0])\nmodel.layer4[1].bn2 = nn.GroupNorm(1, model.layer4[1].bn2.weight.shape[0])\n\nmodel.conv1 = nn.Conv2d(in_size, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\nmodel.fc = nn.Sequential(nn.Linear(512, num_classes))\n\ndevice = \"cuda\" if torch.cuda.is_available() else 'cpu'\n\n#model = SimpleModel(in_size, num_classes)\n\nmodel = model.to(device)\n\noptimizer = optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:04:05.661030Z","iopub.execute_input":"2024-07-17T20:04:05.661284Z","iopub.status.idle":"2024-07-17T20:04:06.080192Z","shell.execute_reply.started":"2024-07-17T20:04:05.661259Z","shell.execute_reply":"2024-07-17T20:04:06.079192Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Train new model for 5 epochs\nepoch_indices = []\nfor epoch in range(1, trainingepochs+1):\n    starttime = time.process_time()\n    # train(resnet, epoch, all_data_train_loader, returnable=False)\n    batches = train(model, epoch, target_train_loader, returnable=True, keep_p=P)\n    print(f\"{batches} batches effected\")\n    epoch_indices.append(batches)\n    test(model, all_data_test_loader, dname=\"All data\")\n    test(model, target_test_loader, dname=\"Forget  \")\n    test(model, nontarget_test_loader, dname=\"Retain  \")\n    print(f\"Time taken: {time.process_time() - starttime}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:04:06.081362Z","iopub.execute_input":"2024-07-17T20:04:06.081652Z","iopub.status.idle":"2024-07-17T20:08:23.083445Z","shell.execute_reply.started":"2024-07-17T20:04:06.081628Z","shell.execute_reply":"2024-07-17T20:08:23.082532Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch: 1 [ 49152]\tLoss: 3.566571[0, 1, 3, 4, 7, 8, 9, 12, 13, 14, 17, 24, 25, 26, 27, 31, 32, 33, 37, 39, 40, 41, 45, 47, 48, 51, 52, 54, 56, 58, 60, 65, 66, 68, 69, 71, 77, 78, 79, 80, 85, 87, 94, 100, 101, 102, 103, 105, 106, 107, 108, 111, 112, 119, 120, 123, 124, 125, 126, 127, 130, 131, 132, 136, 137, 140, 142, 143, 147, 151, 153, 155, 156, 160, 161, 162, 163, 169, 170, 172, 173, 174, 175, 176, 177, 178, 180, 183, 184, 185, 186, 188, 190, 191, 193, 198, 199, 200, 201, 204, 210, 211, 212, 213, 215, 217, 220, 221, 222, 224, 225, 226, 227, 229, 230, 232, 235, 236, 237, 243, 248, 250, 251, 252, 253, 254, 255, 256, 257, 259, 261, 264, 265, 267, 272, 275, 281, 285, 287, 289, 290, 292, 293, 296, 300, 301, 303, 305, 308, 310, 312, 321, 322, 323, 324, 327, 328, 329, 333, 336, 338, 340, 344, 346, 347, 348, 350, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 365, 367, 370, 376, 377, 382, 384, 385, 386, 389, 390, 392, 393, 394, 396, 397, 399, 403, 406, 407, 411, 412, 421, 422, 425, 427, 429, 430, 431, 434, 437, 438, 441, 444, 447, 450, 452, 454, 457, 459, 460, 461, 462, 464, 467, 468, 469, 470, 471, 472, 473, 474, 477, 481, 489, 490, 491, 494, 495, 496, 498, 499, 505, 507, 508, 510, 511, 512, 513, 514, 516, 520, 521, 522, 526, 527, 529, 531, 532, 533, 534, 536, 537, 538, 545, 546, 549, 550, 552, 553, 556, 557, 558, 559, 561, 563, 570, 572, 576, 580, 582, 583, 584, 585, 586, 588, 589, 590, 592, 595, 601, 605, 608, 609, 613, 614, 615, 618, 619, 624, 627, 628, 629, 637, 639, 640, 642, 643, 645, 646, 649, 650, 651, 655, 659, 661, 662, 664, 668, 669, 672, 673, 674, 677, 678, 681, 682, 685, 686, 688, 689, 697, 699, 700, 704, 706, 708, 710, 711, 712, 713, 717, 720, 724, 733, 735, 736, 737, 738, 739, 740, 742, 744, 746, 747, 748, 753, 756, 757, 759, 760, 762, 765, 766, 767, 773, 776, 777, 779] batches effected\nAll data: Mean loss: 0.0586, Accuracy: 1216/10000 (12%)\nForget  : Mean loss: 0.0009, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0577, Accuracy: 1216/9900 (12%)\nTime taken: 102.12196164699999\nEpoch: 2 [ 49152]\tLoss: 3.009853[2, 3, 5, 6, 7, 8, 11, 12, 16, 18, 20, 21, 22, 23, 24, 26, 27, 28, 32, 35, 38, 39, 42, 43, 44, 45, 47, 48, 50, 51, 55, 59, 60, 63, 64, 65, 66, 67, 68, 73, 76, 77, 78, 79, 81, 85, 86, 89, 91, 92, 93, 95, 96, 97, 99, 102, 105, 106, 107, 108, 110, 112, 114, 116, 122, 123, 126, 127, 128, 130, 132, 133, 136, 138, 142, 145, 146, 147, 150, 151, 157, 159, 164, 165, 168, 170, 171, 172, 174, 179, 180, 181, 183, 185, 188, 189, 190, 191, 194, 195, 196, 197, 198, 200, 202, 203, 209, 214, 216, 218, 219, 220, 221, 222, 223, 224, 225, 227, 229, 230, 231, 235, 237, 242, 245, 247, 250, 251, 255, 256, 258, 259, 261, 262, 271, 272, 274, 277, 278, 279, 281, 284, 289, 299, 300, 301, 302, 306, 309, 311, 314, 315, 318, 320, 322, 327, 332, 333, 334, 338, 340, 349, 354, 355, 359, 362, 363, 364, 368, 371, 372, 375, 376, 379, 380, 381, 382, 383, 384, 386, 387, 388, 395, 397, 400, 403, 405, 408, 409, 411, 412, 413, 414, 415, 417, 418, 420, 424, 427, 428, 429, 430, 431, 432, 437, 439, 442, 443, 445, 448, 452, 454, 455, 457, 458, 460, 463, 464, 466, 468, 469, 472, 475, 476, 477, 479, 480, 481, 483, 484, 487, 490, 492, 496, 498, 499, 504, 507, 508, 512, 515, 516, 517, 518, 519, 520, 527, 528, 530, 536, 537, 539, 541, 543, 548, 550, 556, 561, 562, 563, 566, 567, 568, 569, 571, 573, 575, 576, 582, 586, 588, 591, 593, 594, 596, 600, 601, 602, 603, 608, 610, 611, 614, 616, 617, 622, 624, 625, 628, 629, 634, 636, 637, 639, 641, 642, 643, 646, 652, 657, 661, 664, 665, 666, 668, 669, 670, 671, 675, 676, 677, 681, 684, 686, 687, 688, 693, 696, 700, 702, 705, 708, 715, 717, 718, 719, 720, 721, 722, 725, 728, 731, 734, 737, 740, 742, 743, 744, 746, 748, 749, 750, 751, 756, 757, 758, 760, 762, 763, 765, 766, 767, 771, 772, 773, 774, 776, 777, 778, 779, 780] batches effected\nAll data: Mean loss: 0.0507, Accuracy: 2157/10000 (22%)\nForget  : Mean loss: 0.0006, Accuracy: 17/100 (17%)\nRetain  : Mean loss: 0.0501, Accuracy: 2140/9900 (22%)\nTime taken: 98.11006806999998\nEpoch: 3 [ 49152]\tLoss: 2.765043[0, 1, 3, 6, 9, 12, 15, 16, 17, 18, 20, 22, 25, 26, 27, 30, 32, 33, 37, 38, 39, 41, 43, 44, 46, 47, 48, 49, 50, 51, 54, 58, 61, 63, 64, 65, 66, 68, 71, 79, 80, 85, 86, 87, 88, 96, 98, 100, 103, 109, 111, 116, 118, 119, 120, 121, 123, 124, 125, 131, 132, 133, 134, 135, 136, 144, 147, 151, 152, 153, 155, 163, 165, 169, 170, 171, 172, 173, 175, 176, 178, 179, 185, 191, 193, 196, 197, 198, 200, 201, 205, 207, 210, 211, 214, 215, 217, 219, 220, 222, 223, 224, 225, 228, 229, 230, 232, 233, 235, 237, 238, 239, 240, 244, 247, 248, 249, 250, 255, 256, 260, 262, 264, 265, 268, 272, 274, 275, 276, 277, 278, 280, 281, 282, 284, 287, 288, 289, 295, 296, 297, 299, 301, 305, 306, 310, 311, 312, 314, 316, 322, 324, 326, 331, 336, 337, 339, 342, 343, 344, 345, 346, 347, 348, 350, 351, 354, 356, 357, 358, 359, 362, 363, 364, 365, 366, 368, 372, 374, 375, 377, 381, 382, 383, 386, 387, 388, 389, 392, 394, 396, 399, 402, 403, 404, 405, 406, 407, 408, 411, 413, 414, 415, 416, 418, 419, 423, 425, 429, 430, 436, 437, 439, 444, 445, 450, 451, 453, 454, 457, 459, 460, 465, 468, 471, 472, 473, 474, 476, 479, 480, 482, 483, 484, 486, 487, 488, 489, 493, 496, 497, 500, 501, 502, 506, 508, 511, 514, 515, 517, 518, 519, 521, 522, 524, 525, 528, 530, 532, 533, 535, 536, 539, 540, 541, 543, 549, 551, 556, 559, 561, 562, 563, 564, 565, 569, 570, 571, 574, 577, 579, 582, 583, 585, 589, 594, 596, 597, 598, 600, 605, 610, 614, 615, 619, 620, 622, 623, 624, 625, 631, 635, 637, 638, 639, 641, 642, 645, 646, 650, 651, 652, 654, 655, 658, 662, 663, 667, 669, 670, 671, 672, 673, 674, 675, 677, 679, 683, 684, 686, 690, 692, 693, 696, 703, 704, 706, 708, 709, 710, 712, 714, 718, 720, 722, 724, 725, 727, 728, 731, 732, 733, 735, 739, 741, 743, 745, 746, 747, 749, 754, 755, 756, 761, 762, 765, 766, 767, 769, 771, 772, 775, 777, 779, 781] batches effected\nAll data: Mean loss: 0.0449, Accuracy: 2841/10000 (28%)\nForget  : Mean loss: 0.0006, Accuracy: 32/100 (32%)\nRetain  : Mean loss: 0.0443, Accuracy: 2809/9900 (28%)\nTime taken: 101.13936787\nEpoch: 4 [ 49152]\tLoss: 2.726422[1, 2, 3, 5, 9, 10, 11, 13, 14, 15, 19, 23, 25, 26, 27, 29, 30, 32, 33, 34, 35, 36, 39, 40, 43, 48, 49, 50, 51, 57, 60, 63, 65, 72, 74, 76, 77, 78, 79, 80, 82, 83, 86, 88, 89, 90, 94, 95, 96, 99, 100, 103, 106, 108, 112, 113, 114, 118, 119, 123, 124, 125, 126, 127, 128, 131, 132, 133, 136, 137, 138, 140, 141, 142, 144, 147, 151, 157, 159, 160, 162, 166, 168, 171, 172, 181, 183, 184, 186, 188, 189, 192, 193, 195, 197, 199, 203, 205, 206, 209, 212, 216, 217, 223, 225, 228, 230, 231, 232, 233, 235, 237, 238, 241, 242, 247, 248, 254, 256, 257, 261, 262, 265, 266, 267, 272, 273, 274, 275, 276, 277, 279, 281, 282, 283, 284, 285, 288, 292, 293, 297, 300, 307, 308, 310, 311, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 331, 332, 333, 336, 338, 339, 340, 341, 345, 349, 350, 353, 359, 361, 362, 363, 364, 365, 366, 368, 371, 372, 373, 379, 380, 381, 384, 386, 387, 389, 390, 391, 392, 394, 395, 396, 398, 403, 405, 406, 407, 408, 410, 412, 415, 419, 420, 421, 422, 425, 430, 432, 443, 444, 446, 447, 450, 453, 454, 458, 459, 462, 463, 465, 466, 468, 469, 470, 475, 482, 483, 486, 490, 491, 492, 493, 494, 495, 500, 503, 505, 507, 509, 510, 511, 512, 513, 515, 517, 518, 520, 523, 527, 528, 531, 534, 535, 537, 540, 542, 543, 547, 549, 558, 559, 561, 563, 564, 565, 569, 574, 575, 576, 578, 580, 581, 584, 585, 592, 597, 598, 600, 602, 603, 605, 606, 611, 614, 617, 619, 621, 623, 625, 628, 631, 632, 635, 637, 641, 643, 644, 645, 648, 649, 652, 653, 654, 660, 663, 665, 668, 670, 674, 675, 676, 677, 678, 680, 688, 689, 690, 692, 693, 694, 696, 698, 699, 700, 701, 702, 705, 706, 707, 711, 712, 714, 715, 718, 719, 720, 721, 723, 724, 725, 726, 727, 731, 739, 741, 743, 745, 746, 751, 752, 754, 756, 758, 759, 762, 765, 766, 768, 771, 772, 779] batches effected\nAll data: Mean loss: 0.0423, Accuracy: 3218/10000 (32%)\nForget  : Mean loss: 0.0007, Accuracy: 12/100 (12%)\nRetain  : Mean loss: 0.0417, Accuracy: 3206/9900 (32%)\nTime taken: 99.57304746099999\n","output_type":"stream"}]},{"cell_type":"code","source":"path = F\"selective_trained.pt\"\ntorch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, path)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:08:23.084643Z","iopub.execute_input":"2024-07-17T20:08:23.084943Z","iopub.status.idle":"2024-07-17T20:08:23.284164Z","shell.execute_reply.started":"2024-07-17T20:08:23.084918Z","shell.execute_reply":"2024-07-17T20:08:23.283232Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"path = F\"selective_trained.pt\"\ncheckpoint = torch.load(path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:08:23.285331Z","iopub.execute_input":"2024-07-17T20:08:23.285612Z","iopub.status.idle":"2024-07-17T20:08:23.383106Z","shell.execute_reply.started":"2024-07-17T20:08:23.285588Z","shell.execute_reply":"2024-07-17T20:08:23.382211Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#for i in range(1, trainingepochs+1):\n#    for j in epoch_indices[i-1]:\n#        path = f\"steps/e{i}b{j:04}.pkl\"\n#        steps = torch.load(path)\n#        print(f\"\\rLoading steps/e{i}b{j:04}.pkl\", end=\"\")\n#        const = 1\nwith torch.no_grad():\n    for key, param in model.named_parameters():\n        param -= stuff[key].to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:08:23.384248Z","iopub.execute_input":"2024-07-17T20:08:23.384511Z","iopub.status.idle":"2024-07-17T20:08:23.404024Z","shell.execute_reply.started":"2024-07-17T20:08:23.384488Z","shell.execute_reply":"2024-07-17T20:08:23.403283Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test(model, all_data_test_loader, dname=\"All data\")\ntest(model, target_test_loader, dname=\"Forget  \")\ntest(model, nontarget_test_loader, dname=\"Retain  \")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:08:23.405130Z","iopub.execute_input":"2024-07-17T20:08:23.405512Z","iopub.status.idle":"2024-07-17T20:08:30.268118Z","shell.execute_reply.started":"2024-07-17T20:08:23.405478Z","shell.execute_reply":"2024-07-17T20:08:30.267196Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"All data: Mean loss: 0.0447, Accuracy: 3019/10000 (30%)\nForget  : Mean loss: 0.0012, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0437, Accuracy: 3019/9900 (30%)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor(0.305, device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"path = F\"selective_post_trained.pt\"\ntorch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, path)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:08:30.269352Z","iopub.execute_input":"2024-07-17T20:08:30.269641Z","iopub.status.idle":"2024-07-17T20:08:30.462343Z","shell.execute_reply.started":"2024-07-17T20:08:30.269615Z","shell.execute_reply":"2024-07-17T20:08:30.461525Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"path = F\"selective_post_trained.pt\"\ncheckpoint = torch.load(path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:08:30.463493Z","iopub.execute_input":"2024-07-17T20:08:30.463789Z","iopub.status.idle":"2024-07-17T20:08:30.595016Z","shell.execute_reply.started":"2024-07-17T20:08:30.463764Z","shell.execute_reply":"2024-07-17T20:08:30.594022Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Train model for 10 epochs\nfor epoch in range(trainingepochs+1,trainingepochs+forgetfulepochs+1):\n  # train(resnet, epoch, nonthree_train_loader, returnable=False)\n    _ = train(model, epoch, nontarget_train_loader, returnable=True)\n    test(model, all_data_test_loader, dname=\"All data\")\n    test(model, target_test_loader, dname=\"Forget  \")\n    test(model, nontarget_test_loader, dname=\"Retain  \")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:08:30.596507Z","iopub.execute_input":"2024-07-17T20:08:30.597232Z","iopub.status.idle":"2024-07-17T20:10:20.933059Z","shell.execute_reply.started":"2024-07-17T20:08:30.597197Z","shell.execute_reply":"2024-07-17T20:10:20.932134Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch: 5 [ 49152]\tLoss: 2.096139All data: Mean loss: 0.0403, Accuracy: 3507/10000 (35%)\nForget  : Mean loss: 0.0013, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0392, Accuracy: 3507/9900 (35%)\nEpoch: 6 [ 49152]\tLoss: 1.976110All data: Mean loss: 0.0388, Accuracy: 3833/10000 (38%)\nForget  : Mean loss: 0.0016, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0374, Accuracy: 3833/9900 (39%)\nEpoch: 7 [ 49152]\tLoss: 1.570183All data: Mean loss: 0.0391, Accuracy: 3920/10000 (39%)\nForget  : Mean loss: 0.0017, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0377, Accuracy: 3920/9900 (40%)\nEpoch: 8 [ 49152]\tLoss: 1.563240All data: Mean loss: 0.0406, Accuracy: 3873/10000 (39%)\nForget  : Mean loss: 0.0019, Accuracy: 0/100 (0%)\nRetain  : Mean loss: 0.0390, Accuracy: 3873/9900 (39%)\n","output_type":"stream"}]}]}